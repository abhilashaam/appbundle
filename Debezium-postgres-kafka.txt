Initial Setup
To keep this tutorial isolated from other application running in your Kubernetes cluster and to cleanup easier, we will create a separate namespace for the new resources.

  $ kubectl create namespace kafka-connect
(Optional) You may configure your Kubernetes context's default namespace to kafka-connect using the following command:


  $ kubectl config set-context --current --namespace kafka-connect

Install Kafka & Zookeeper
This section will guide you through the installation of Kafka & Zookeeper. You will also deploy a Kafka client pod to interact with the Kafka Cluster, as well as configure 3 Kafka Topics that will be used by Kafka Connect.

Install Kafka & Zookeeper to your namespace using the Incubator Helm Chart.


  $ helm upgrade --install kafka --namespace kafka-connect incubator/kafka --set external.enabled=true
Deploy a Kafka Connect client container to your cluster by creating a file in your workspace named kafka-client-deploy.yaml with the following contents:


# kafka-client-deploy.yaml

apiVersion: v1
kind: Pod
metadata:
  name: kafka-client
spec:
  containers:
  - name: kafka-client
    image: confluentinc/cp-kafka:5.0.1
    command:
      - sh
      - -c
      - "exec tail -f /dev/null"
Execute the following command to deploy the Kafka Client Pod:


$ kubectl create -f kafka-client-deploy.yaml -n kafka-connect
Create the Kafka Connect Topics using the following commands:


$ kubectl -n kafka-connect exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-offsets --create --partitions 1 --replication-factor 1

$ kubectl -n kafka-connect exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-configs --create --partitions 1 --replication-factor 1
Copied to clipboard
$ kubectl -n kafka-connect exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --topic connect-status --create --partitions 1 --replication-factor 1
Install Kafka Connect
This section will guide you through the installation of Kafka Connect using the Debezium Kafka Connect Docker Image. As part of this installation, you will create a NodePort service to expose the Kafka Connect API. This service will be available on port 30500 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30500.

Create a file named kafka-connect-deploy.yaml in your workspace and add the following contents:

  # kafka-connect-deploy.yaml

  apiVersion: apps/v1
  kind: Deployment
  metadata:
    name: kafkaconnect-deploy
    labels:
      app: kafkaconnect
  spec:
    replicas: 1
    selector:
      matchLabels:
        app: kafkaconnect
    template:
      metadata:
        labels:
          app: kafkaconnect
      spec:
        containers:
          - name: kafkaconnect-container
            image: debezium/connect:0.10.0.CR1
            readinessProbe:
              httpGet:
                path: /
                port: 8083
            livenessProbe:
              httpGet:
                path: /
                port: 8083
            env:
              - name: BOOTSTRAP_SERVERS
                value: kafka:9092
              - name: GROUP_ID
                value: "1"
              - name: OFFSET_STORAGE_TOPIC
                value: connect-offsets
              - name: CONFIG_STORAGE_TOPIC
                value: connect-configs
              - name: STATUS_STORAGE_TOPIC
                value: connect-status
            ports:
            - containerPort: 8083
  ---
  apiVersion: v1
  kind: Service
  metadata:
    name: kafkaconnect-service
    labels:
      app: kafkaconnect-service
  spec:
    type: NodePort
    ports:
      - name: kafkaconnect
        protocol: TCP
        port: 8083
        nodePort: 30500
    selector:
        app: kafkaconnect
Deploy Kafka Connect with the following command:

  $ kubectl create -f kafka-connect-deploy.yaml --namespace kafka-connect
Install PostgreSQL
This section will guide you through the installation of PostgreSQL using the Stable Helm Chart. You will also add some additional configuration for PostgreSQL necessary for Debezium to read the PostgreSQL transaction log. PostgreSQL will be available on port 30600 of your cluster nodes. If you are using Docker Desktop, this will be http://localhost:30600.

Create a PostgreSQL configuration necessary for Debezium. Create a file in your workspace named extended.conf with the following contents:


  # extended.conf

  wal_level = logical
  max_wal_senders = 1
  max_replication_slots = 1
Create a ConfigMap from the extended.conf file with the following command:


  $ kubectl create configmap --namespace kafka-connect --from-file=extended.conf postgresql-config
Install PostgreSQL using the Stable Helm Chart with the following command:


  $ helm install postgres --namespace kafka-connect --set extendedConfConfigMap=postgresql-config --set service.type=NodePort --set service.nodePort=30600 --set postgresqlPassword=passw0rd stable/postgresql
Add Sample Data to PostgreSQL
Open a shell in the Postgres container.


  $ kubectl exec --namespace kafka-connect -it postgres-postgresql-0  -- /bin/sh
Login to Postgres with the following command, entering the password passw0rd when prompted.


  $ psql --user postgres
Create a table named containers.


  CREATE TABLE containers(containerid VARCHAR(30) NOT NULL,type VARCHAR(20),status VARCHAR(20),brand VARCHAR(50),capacity DECIMAL,CREATIONDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP,UPDATEDATE TIMESTAMP DEFAULT CURRENT_TIMESTAMP,PRIMARY KEY (containerid));
Insert data into the table.


  INSERT INTO containers (containerid, type, status, brand, capacity) VALUES ('C01','Reefer','Operational','containerbrand',20), 
('C02','Dry','Operational','containerbrand',20), ('C03','Dry','Operational','containerbrand',40),
 ('C04','FlatRack','Operational','containerbrand',40), ('C05','OpenTop','Operational','containerbrand',40), 
('C06','OpenSide','Operational','containerbrand',40), ('C07','Tunnel','Operational','containerbrand',40), 
('C08','Tank','Operational','containerbrand',40), ('C09','Thermal','Operational','containerbrand',20);
Configure the Debezium PostgreSQL connector
This section will show you how to configure the Debezium PostgreSQL connector.

Using your HTTP client (cURL shown), make the following request to the Kafka Connect API. This will configure a new Debezium PostgreSQL connector. This connector monitors the pgoutput stream for operations on the containers table.
Note: If you are not using Docker Desktop, please set localhost to the hostname/IP of one of your cluster nodes.

Note: If you did not follow the Add Sample Data to PostgreSQL section, replace "public.containers" with the name of your table.


curl -X POST \
  http://20.62.218.48:8083/connectors \
  -H 'Content-Type: application/json' \
  -d '{
    "name": "containers-connector",
    "config": {
            "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
            "plugin.name": "pgoutput",
            "database.hostname": "postgres-postgresql",
            "database.port": "5432",
            "database.user": "postgres",
            "database.password": "passw0rd",
            "database.dbname": "postgres",
            "database.server.name": "postgres",
            "table.whitelist": "public.containers"
      }
}'
List the Kafka Topics, showing your newly created topic


  $ kubectl -n kafka-connect exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --list
Tail the Kafka postgres.public.containers topic to show database transactions being written to the topic from Kafka Connect.

Note: Change postgres.public.containers if you are not using the sample database data


  $ kubectl -n kafka-connect exec kafka-client -- kafka-console-consumer --topic postgres.public.containers --from-beginning --bootstrap-server kafka:9092
You may continue to make Create, Update and Delete transactions to the containers table, these changes will appear as messages in the Kafka topic.

Cleanup
This section will help you remove all of the resources created during this tutorial.

Delete the Kafka Helm Release


  $ helm delete kafka --purge
Delete the PostgreSQL Helm Release


  $ helm delete postgres --purge
Delete the kafka-connect namespace


  $ kubectl delete namespace kafka-connect


*************** NEW ********************

https://ehsaniara.medium.com/cdc-with-postgres-debezium-to-kafka-strimzi-bf9212ae9d78
https://strimzi.io/blog/2021/03/29/connector-build/
https://strimzi.io/blog/2020/01/27/deploying-debezium-with-kafkaconnector-resource/
https://stackoverflow.com/questions/64919947/how-to-use-kafka-connect-in-strimzi

----refer---> https://strimzi.io/blog/2021/03/29/connector-build/
	https://strimzi.io/blog/2020/01/27/deploying-debezium-with-kafkaconnector-resource/
---debezium error discussion---> https://groups.google.com/g/debezium/c/6k_PVrWsdYI?pli=1


curl -L https://github.com/strimzi/strimzi-kafka-operator/releases/download/0.28.0/strimzi-cluster-operator-0.28.0.yaml \
  | sed 's/namespace: .*/namespace: kafka/' \
  | kubectl apply -f - -n kafka 

kubectl -n kafka \
    apply -f https://raw.githubusercontent.com/strimzi/strimzi-kafka-operator/0.28.0/examples/kafka/kafka-persistent-single.yaml \
  && kubectl wait kafka/my-cluster --for=condition=Ready --timeout=300s -n kafka


https://repo1.maven.org/maven2/io/debezium/debezium-connector-postgres/1.0.0.Final/debezium-connector-postgres-1.0.0.Final-plugin.tar.gz

cat <<EOF > debezium-postgres-credentials.properties
postgres_username: data_engineer
postgres_password: password
EOF
kubectl -n kafka create secret generic postgres-credentials \
  --from-file=debezium-postgres-credentials.properties
rm debezium-postgres-credentials.properties

########## shivaswaroopk/debezium-postgres

cat <<EOF | kubectl -n kafka apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:
  image: ehsaniara/postgres-connect-debezium:latest
  replicas: 1
  bootstrapServers: my-cluster-kafka-bootstrap:9093
  tls:
    trustedCertificates:
      - secretName: my-cluster-cluster-ca-cert
        certificate: ca.crt
  config:
    config.storage.replication.factor: 1
    offset.storage.replication.factor: 1
    status.storage.replication.factor: 1
    config.providers: file
    config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider
  externalConfiguration:
    volumes:
      - name: connector-config
        secret:
          secretName: postgres-credentials

EOF

>>>> kubectl -n kafka port-forward service/my-connect-cluster-connect-api 8083:8083

cat <<EOF | kubectl -n kafka apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
    strimzi.io/use-connector-resources: "true"
spec:  
  image: ehsaniara/postgres-connect-debezium:latest
  replicas: 1
  bootstrapServers: my-cluster-kafka-bootstrap:9092
  config:
    group.id: my-connect-cluster
    offset.storage.topic: my-connect-cluster-offsets
    config.storage.topic: my-connect-cluster-configs
    status.storage.topic: my-connect-cluster-status
    key.converter: org.apache.kafka.connect.json.JsonConverter
    value.converter: org.apache.kafka.connect.json.JsonConverter
    key.converter.schemas.enable: true
    value.converter.schemas.enable: true
    config.storage.replication.factor: 1
    offset.storage.replication.factor: 1
    status.storage.replication.factor: 1
  externalConfiguration:
    volumes:
      - name: connector-config
        secret:
          secretName: postgres-credentials
EOF
  


curl --location --request POST 'http://127.0.0.1:8083/connectors/' \
--header 'Accept: application/json' \
--header 'Content-Type: application/json' \
--data-raw '{
    "name": "sde-connector",
    "config": {
        "connector.class": "io.debezium.connector.postgresql.PostgresConnector",
        "database.hostname": "postgres.postgres",
        "database.port": "5432",
        "tasks.max": "1",
        "database.user": "data_engineer",
        "database.password": "password",
        "database.dbname": "data_engineer",
        "database.server.name": "server1",
        "database.history.kafka.bootstrap.servers": "my-cluster-kafka-bootstrap:9093",
        "database.history.kafka.topic": "schema-changes.inventory",
        "database.whitelist": "inventory",
        "include.schema.changes": true,
        "database.server.id": "184054"
    }
}'

cat <<EOF | kubectl -n kafka apply -f -
apiVersion: kafka.strimzi.io/v1beta2
kind: KafkaConnector
metadata:
  name: inventory-connector
  labels:
    strimzi.io/cluster: my-connect-cluster
spec:
  class: "io.debezium.connector.postgresql.PostgresConnector"
  tasksMax: 1
  config:
    database.hostname: "20.85.146.225"
    database.port: "5432"
    database.user: "postgres"
    database.password: "assetmark"
    database.server.id: "184054"
    database.server.name: "dbserver1"
    database.whitelist: "inventory"
    database.history.kafka.bootstrap.servers: "my-cluster-kafka-bootstrap:9092"
    database.history.kafka.topic: "schema-changes.inventory"
    include.schema.changes: true 
EOF

>>>  kubectl -n kafka get kctr inventory-connector -o yaml

## database.hostname: 20.124.139.203
######### error ---> message: |-
      PUT /connectors/inventory-connector/config returned 400 (Bad Request): Connector configuration is invalid and contains the following 1 error(s):
      A value is required
      You can also find the above list of errors at the endpoint `/connector-plugins/{connectorType}/config/validate`
    reason: ConnectRestException




curl -s -X PUT -H "Content-Type:application/json” \
my-connect-cluster-connect-api:8083/connector-plugins/PostgresConnector/config/validate/ \
-d '{ "name": "inventory-connector", "connector.class": "io.debezium.connector.postgresql.PostgresConnector", "tasks.max": "1", "database.hostname": "postgres.postgres", "database.port": "5432", "database.user": "data_engineer", "database.password": "password", "database.server.id": "184054", "database.server.name": "dbserver1", "database.whitelist": "inventory", "database.history.kafka.bootstrap.servers": "my-cluster-kafka-bootstrap:9092", "database.history.kafka.topic": "dbhistory.inventory" }’

>>> kubectl exec --namespace postgres -it postgres-deployment-69c757d5c8-6bljf  -- /bin/sh

## kubectl -n kafka exec kafka-client -- kafka-topics --zookeeper kafka-zookeeper:2181 --list
###kubectl -n kafka exec my-cluster-kafka-0 -- kafka-topics --zookeeper my-cluster-zookeeper-client:2181 --list

## kubectl -n kafka exec my-cluster-kafka-0 -c kafka -i -t --   bin/kafka-console-consumer.sh --bootstrap-server 127.0.0.1:9092 --topic server1.inventory.customers

INSERT INTO customers (ID,DEPT,CUS_ID) VALUES (1, 'TESTING', 1234);


status:
  conditions:
  - lastTransitionTime: "2022-04-19T09:51:59.084723Z"
    message: 'PUT /connectors/inventory-connector/config returned 500 (Internal Server
      Error): Failed to find any class that implements Connector and which name matches
      io.debezium.connector.postgresql.PostgresConnector, available connectors are:
      PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSinkConnector,
      name=''org.apache.kafka.connect.file.FileStreamSinkConnector'', version=''2.5.0'',
      encodedVersion=2.5.0, type=sink, typeName=''sink'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.file.FileStreamSourceConnector,
      name=''org.apache.kafka.connect.file.FileStreamSourceConnector'', version=''2.5.0'',
      encodedVersion=2.5.0, type=source, typeName=''source'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorCheckpointConnector,
      name=''org.apache.kafka.connect.mirror.MirrorCheckpointConnector'', version=''1'',
      encodedVersion=1, type=source, typeName=''source'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorHeartbeatConnector,
      name=''org.apache.kafka.connect.mirror.MirrorHeartbeatConnector'', version=''1'',
      encodedVersion=1, type=source, typeName=''source'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.mirror.MirrorSourceConnector,
      name=''org.apache.kafka.connect.mirror.MirrorSourceConnector'', version=''1'',
      encodedVersion=1, type=source, typeName=''source'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.tools.MockConnector, name=''org.apache.kafka.connect.tools.MockConnector'',
      version=''2.5.0'', encodedVersion=2.5.0, type=connector, typeName=''connector'',
      location=''classpath''}, PluginDesc{klass=class org.apache.kafka.connect.tools.MockSinkConnector,
      name=''org.apache.kafka.connect.tools.MockSinkConnector'', version=''2.5.0'',
      encodedVersion=2.5.0, type=sink, typeName=''sink'', location=''classpath''},
      PluginDesc{klass=class org.apache.kafka.connect.tools.MockSourceConnector, name=''org.apache.kafka.connect.tools.MockSourceConnector'',


message: |-
      PUT /connectors/inventory-connector/config returned 400 (Bad Request): Connector configuration is invalid and contains the following 1 error(s):
      A value is required
      You can also find the above list of errors at the endpoint `/connector-plugins/{connectorType}/config/validate`
    reason: ConnectRestException

#### Delete Evicted pods
kubectl get pod -n nginx-test | grep Evicted | awk '{print $1}' | xargs kubectl delete pod -n nginx-test
